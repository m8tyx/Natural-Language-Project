{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f63a761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Data wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "# üìä Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# üìù NLP - NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# ‚ö° NLP - spaCy\n",
    "import spacy\n",
    "import contractions \n",
    "\n",
    "# ü§ñ Machine Learning tools\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from lightgbm import LGBMClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, DataCollatorWithPadding \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3693baf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "330cd601",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Arknights.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fe60c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cleaning\n",
    "def cleaningText(text):\n",
    "    # hapus link\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "    # hapus HTML entities\n",
    "    text = re.sub(r\"&\\w+;\", ' ', text)\n",
    "    # hapus karakter non-ASCII kecuali emoji\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    # hapus tanda baca kecuali apostrof (biar kontraksi bisa di-expand)\n",
    "    text = re.sub(r\"[^\\w\\s']\", '', text)\n",
    "    # ubah newline ke spasi\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    # hapus spasi berlebihan\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# 2. Casefolding\n",
    "def casefoldingText(text):\n",
    "    return text.casefold()  # lebih aman dari lower()\n",
    "\n",
    "# Fungsi expand contraction\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "# 3. Tokenizing\n",
    "def tokenizingText(text):\n",
    "    # Tokenisasi\n",
    "    tokens = word_tokenize(text)\n",
    "    # Hapus apostrof tunggal yang tersisa (misal 's atau 're)\n",
    "    tokens = [token.replace(\"'\", \"\") for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "# 4. Stopwords Filtering\n",
    "stop_words = set(stopwords.words('english'))\n",
    "custom_stopwords = {\n",
    "    \"time\", \"one\", \"get\", \"make\", \"go\", \"play\", \"really\", \"still\", \"even\", \"well\",\n",
    "    \"much\", \"many\", \"lot\", \"thing\", \"stuff\", \"something\", \"anything\", \"everything\", \"nothing\",\n",
    "    \"also\", \"maybe\", \"actually\", \"probably\", \"almost\", \"kinda\", \"sort\", \"bit\", \"very\",\n",
    "    \"etc\", \"like\", \"just\", \"pretty\", \"rather\", \"quite\", \"somehow\",\n",
    "    \"game\", \"app\", \"application\", \"gameplay\", \"character\", \"horse\", \"girl\", \"story\",\n",
    "    \"graphics\", \"system\", \"feature\", \"update\", \"dev\", \"developer\", \"version\",\n",
    "    \"can\", \"could\", \"would\", \"should\", \"may\", \"might\", \"will\", \"shall\", \"must\",\n",
    "    \"i\", \"me\", \"my\", \"we\", \"us\", \"our\", \"you\", \"your\", \"they\", \"them\", \"their\", \"he\", \"she\", \"it\", \"its\"\n",
    "}\n",
    "stop_words.update(custom_stopwords)\n",
    "def filteringText(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# 5. Slang Normalization\n",
    "# Fungsi untuk baca slang dari file txt\n",
    "def load_slang_dict(filepath=\"slang.txt\"):\n",
    "    slang_dict = {}\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if \"=\" in line:  # pastikan format benar\n",
    "                key, value = line.split(\"=\", 1)\n",
    "                slang_dict[key.strip().lower()] = value.strip()\n",
    "    return slang_dict\n",
    "\n",
    "# Fungsi normalisasi teks\n",
    "def normalize_text(text, slang_dict):\n",
    "    # urutkan slang dari yang panjang ke pendek supaya frasa lebih dulu diproses\n",
    "    for slang in sorted(slang_dict.keys(), key=len, reverse=True):\n",
    "        pattern = r\"\\b\" + re.escape(slang) + r\"\\b\"\n",
    "        text = re.sub(pattern, slang_dict[slang], text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "slang_dict = load_slang_dict(\"slang.txt\")\n",
    "\n",
    "# 6. Lemmatization (spaCy)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def lemmatizationText(tokens):\n",
    "    doc = nlp(\" \".join(tokens))  # spaCy butuh kalimat\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "# 7. To sentence\n",
    "def toSentence(list_words):\n",
    "    return \" \".join(word for word in list_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1a344e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:48<00:00, 205.42it/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_review(text, slang_dict):\n",
    "    text = cleaningText(text)            # 1. Cleaning\n",
    "    text = casefoldingText(text)         # 2. Casefolding\n",
    "    text = expand_contractions(text)     # 3. Expand contractions (üî• di sini)\n",
    "    text = normalize_text(text, slang_dict)  # 4. Slang normalization\n",
    "    tokens = tokenizingText(text)        # 5. Tokenizing\n",
    "    tokens = filteringText(tokens)       # 6. Stopword filtering\n",
    "    tokens = lemmatizationText(tokens)   # 7. Lemmatization\n",
    "    return toSentence(tokens)            # 8. To sentence\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()  # aktifkan tqdm untuk pandas\n",
    "\n",
    "df['text_final'] = df['content'].fillna(\"\").progress_apply(lambda x: preprocess_review(x, slang_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12a9aaba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good</td>\n",
       "      <td>2970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gacha</td>\n",
       "      <td>2385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>play</td>\n",
       "      <td>1521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>great</td>\n",
       "      <td>1270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>love</td>\n",
       "      <td>1209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>anime</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>attempt</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>phone</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>lose</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>enough</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       word  count\n",
       "0      good   2970\n",
       "1     gacha   2385\n",
       "2      play   1521\n",
       "3     great   1270\n",
       "4      love   1209\n",
       "..      ...    ...\n",
       "95    anime    224\n",
       "96  attempt    223\n",
       "97    phone    221\n",
       "98     lose    215\n",
       "99   enough    215\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def get_most_common_words(texts, top_n=100):\n",
    "    all_tokens = []\n",
    "    for text in texts:\n",
    "        cleaned = cleaningText(text)          # pakai fungsi cleaning kamu\n",
    "        lowered = casefoldingText(cleaned)\n",
    "        tokens = tokenizingText(lowered)\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "    # hitung frekuensi kata\n",
    "    counter = Counter(all_tokens)\n",
    "    return counter.most_common(top_n)\n",
    "\n",
    "\n",
    "# Ambil kolom review\n",
    "reviews = df[\"text_final\"].dropna().tolist()\n",
    "\n",
    "# Cari kata paling sering muncul\n",
    "common_words = get_most_common_words(reviews, top_n=100)\n",
    "\n",
    "# Masukkan ke DataFrame supaya rapi\n",
    "df_freq = pd.DataFrame(common_words, columns=[\"word\", \"count\"])\n",
    "\n",
    "# Lihat hasil\n",
    "df_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41e1ceeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "positive    7016\n",
      "neutral     1750\n",
      "negative    1234\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "\n",
    "# Inisialisasi VADER\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Update lexicon supaya kata slang yang sudah dinormalisasi punya bobot sentimen\n",
    "custom_words = {\n",
    "    \"generous\": 2.5,   # positif\n",
    "    \"stingy\": -2.5,    # negatif\n",
    "}\n",
    "\n",
    "sia.lexicon.update(custom_words)\n",
    "\n",
    "# Contoh fungsi untuk kasih label\n",
    "def get_sentiment(text):\n",
    "    score = sia.polarity_scores(text)['compound']  # ambil skor compound\n",
    "    if score >= 0.05:\n",
    "        return \"positive\"\n",
    "    elif score <= -0.05:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# Contoh: buat kolom label dari review\n",
    "df['label'] = df['text_final'].apply(get_sentiment)\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fec04c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"text_final\"]\n",
    "y = df[\"label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88c09d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Percobaan 1: SVM + TF-IDF + 80/20 ===\n",
      "Train Accuracy: 0.96375\n",
      "Test Accuracy: 0.904\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.62      0.70       237\n",
      "     neutral       0.81      0.90      0.85       343\n",
      "    positive       0.94      0.95      0.95      1420\n",
      "\n",
      "    accuracy                           0.90      2000\n",
      "   macro avg       0.85      0.82      0.83      2000\n",
      "weighted avg       0.90      0.90      0.90      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Percobaan 1: SVM + TF-IDF + 80/20 ===\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "svm_model = SVC(kernel=\"linear\", random_state=42)\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Akurasi training\n",
    "y_train_pred = svm_model.predict(X_train_tfidf)\n",
    "train_acc_svm = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Train Accuracy:\", train_acc_svm)\n",
    "\n",
    "# Akurasi testing\n",
    "y_test_pred = svm_model.predict(X_test_tfidf)\n",
    "test_acc_svm = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test Accuracy:\", test_acc_svm)\n",
    "\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d6e4199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Percobaan 2: LightGBM + TF-IDF + 75/25 ===\n",
      "Sebelum SMOTE: [ 925 1313 5262]\n",
      "Sesudah SMOTE: [5262 5262 5262]\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027094 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 48262\n",
      "[LightGBM] [Info] Number of data points in the train set: 15786, number of used features: 1551\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\Documents\\data\\Proyek-NNL\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9924616749018117\n",
      "Test Accuracy: 0.914\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.71      0.74       309\n",
      "     neutral       0.87      0.91      0.89       437\n",
      "    positive       0.95      0.95      0.95      1754\n",
      "\n",
      "    accuracy                           0.91      2500\n",
      "   macro avg       0.87      0.86      0.86      2500\n",
      "weighted avg       0.91      0.91      0.91      2500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\Documents\\data\\Proyek-NNL\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Percobaan 2: LightGBM + TF-IDF + 75/25 ===\")\n",
    "# Encode y (label)\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)  # ['negative','neutral','positive'] -> [0,1,2]\n",
    "\n",
    "# === Split data ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.25, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# === TF-IDF ===\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2))  # bisa diatur\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# === SMOTE ===\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"Sebelum SMOTE:\", np.bincount(y_train))\n",
    "print(\"Sesudah SMOTE:\", np.bincount(y_train_res))\n",
    "\n",
    "# === LightGBM + Hyperparameter Tuning ===\n",
    "param_dist = {\n",
    "    \"n_estimators\": [200, 500, 800],\n",
    "    \"learning_rate\": [0.05, 0.1, 0.2],\n",
    "    \"max_depth\": [10, 20, -1],  \n",
    "    \"num_leaves\": [31, 63, 127],\n",
    "    \"subsample\": [0.7, 0.9, 1.0],\n",
    "    \"colsample_bytree\": [0.7, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "lgbm = LGBMClassifier(random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=lgbm,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Training\n",
    "random_search.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Evaluasi\n",
    "best_lgbm = random_search.best_estimator_\n",
    "y_train_pred = best_lgbm.predict(X_train_res)\n",
    "train_acc_lgbm = accuracy_score(y_train_res, y_train_pred)\n",
    "print(\"Train Accuracy:\", train_acc_lgbm)\n",
    "\n",
    "# Prediksi pada testing set (gunakan X_test_tfidf, bukan X_test)\n",
    "y_test_pred = best_lgbm.predict(X_test_tfidf)\n",
    "test_acc_lgbm = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test Accuracy:\", test_acc_lgbm)\n",
    "\n",
    "print(classification_report(y_test, y_test_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb31bdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Percobaan 3 : BERT Fine Tunned + 80/20 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8000/8000 [00:00<00:00, 14459.71 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [00:00<00:00, 6098.88 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_6112\\1860837187.py:45: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\andre\\Documents\\data\\Proyek-NNL\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 2:47:27, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.227800</td>\n",
       "      <td>0.239019</td>\n",
       "      <td>0.927500</td>\n",
       "      <td>0.926091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.136800</td>\n",
       "      <td>0.223023</td>\n",
       "      <td>0.942500</td>\n",
       "      <td>0.942353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.069100</td>\n",
       "      <td>0.243055</td>\n",
       "      <td>0.947000</td>\n",
       "      <td>0.946562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\Documents\\data\\Proyek-NNL\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\andre\\Documents\\data\\Proyek-NNL\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\andre\\Documents\\data\\Proyek-NNL\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.987875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\Documents\\data\\Proyek-NNL\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.947\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.83      0.84       247\n",
      "     neutral       0.95      0.89      0.92       350\n",
      "    positive       0.96      0.98      0.97      1403\n",
      "\n",
      "    accuracy                           0.95      2000\n",
      "   macro avg       0.92      0.90      0.91      2000\n",
      "weighted avg       0.95      0.95      0.95      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Percobaan 3 : BERT Fine Tunned + 80/20 ===\")\n",
    "# 1. Encode label\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)  # ['negative','neutral','positive'] -> [0,1,2]\n",
    "\n",
    "# 2. Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# 3. HuggingFace Dataset\n",
    "df_train = pd.DataFrame({'text': X_train, 'label': y_train})\n",
    "df_test = pd.DataFrame({'text': X_test, 'label': y_test})\n",
    "\n",
    "train_ds = Dataset.from_pandas(df_train)\n",
    "test_ds = Dataset.from_pandas(df_test)\n",
    "\n",
    "# 4. Tokenizer + preprocessing\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def preprocess(batch):\n",
    "    return tokenizer(batch['text'], truncation=True, padding=True)  # dynamic padding\n",
    "\n",
    "train_ds = train_ds.map(preprocess, batched=True)\n",
    "test_ds = test_ds.map(preprocess, batched=True)\n",
    "\n",
    "# 5. Model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', \n",
    "    num_labels=3\n",
    ")\n",
    "\n",
    "# 6. Data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# 7. Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "# 8. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# 9. Train model\n",
    "trainer.train()\n",
    "\n",
    "# 10. Evaluate\n",
    "# Training set\n",
    "train_pred = trainer.predict(train_ds)\n",
    "y_train_true = train_pred.label_ids\n",
    "y_train_pred = train_pred.predictions.argmax(-1)\n",
    "print(\"Train Accuracy:\", accuracy_score(y_train_true, y_train_pred))\n",
    "\n",
    "# Testing set\n",
    "test_pred = trainer.predict(test_ds)\n",
    "y_test_true = test_pred.label_ids\n",
    "y_test_pred = test_pred.predictions.argmax(-1)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test_true, y_test_pred))\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test_true, y_test_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0938cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_bert = accuracy_score(y_train_true, y_train_pred)\n",
    "test_acc_bert = accuracy_score(y_test_true, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a917c64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Model  Train Accuracy  Test Accuracy\n",
      "0       SVM + TF-IDF + 80/20        0.963750          0.904\n",
      "1  LightGBM + TF-IDF + 75/25        0.992462          0.914\n",
      "2    BERT Fine-tuned + 80/20        0.987875          0.947\n"
     ]
    }
   ],
   "source": [
    "# Simpan hasil tiap model\n",
    "results_summary = []\n",
    "\n",
    "# SVM\n",
    "results_summary.append({\n",
    "    \"Model\": \"SVM + TF-IDF + 80/20\",\n",
    "    \"Train Accuracy\": train_acc_svm,\n",
    "    \"Test Accuracy\": test_acc_svm\n",
    "})\n",
    "\n",
    "# LightGBM\n",
    "results_summary.append({\n",
    "    \"Model\": \"LightGBM + TF-IDF + 75/25\",\n",
    "    \"Train Accuracy\": train_acc_lgbm,\n",
    "    \"Test Accuracy\": test_acc_lgbm\n",
    "})\n",
    "\n",
    "# BERT Fine-tuned\n",
    "results_summary.append({\n",
    "    \"Model\": \"BERT Fine-tuned + 80/20\",\n",
    "    \"Train Accuracy\": train_acc_bert,\n",
    "    \"Test Accuracy\": test_acc_bert\n",
    "})\n",
    "\n",
    "# Tampilkan sebagai tabel\n",
    "df_summary = pd.DataFrame(results_summary)\n",
    "print(df_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b635e990",
   "metadata": {},
   "source": [
    "![Review Screenshot](https://i.imgur.com/RuzIWMq.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d93506e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: positive\n"
     ]
    }
   ],
   "source": [
    "# Teks baru\n",
    "text = \"I've generally enjoyed this game and it's story. The downside of it is that some stages are so tightly tuned that it starts to feel more like a puzzle game than a TD unless you happen to have a few strong units. I still haven't gotten caught up with the story due to the difficulty but probably will some day.\"\n",
    "\n",
    "# Tokenisasi\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Prediksi\n",
    "outputs = model(**inputs)\n",
    "pred = outputs.logits.argmax(dim=1).item()\n",
    "\n",
    "# Konversi kembali ke label asli\n",
    "label = le.inverse_transform([pred])[0]\n",
    "print(\"Predicted label:\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f642ac",
   "metadata": {},
   "source": [
    "![Review Screenshot](https://i.imgur.com/311envQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f8420946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: negative\n"
     ]
    }
   ],
   "source": [
    "# Teks baru\n",
    "text = \"Just got my acc disappear, thx üï∫Contacted the cs, after I have found the IGN#XXXX they still couldn't recover my acc, cuz they couldn't find it. I sent a screenshots of my acc. Even then they couldn't.. I was able to find, that the support is ai slop, that basically a chatbot that costs a 50-80 buck a month. So if anyone have had some hopes to recover the accs -- just forget about it.\"\n",
    "\n",
    "# Tokenisasi\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Prediksi\n",
    "outputs = model(**inputs)\n",
    "pred = outputs.logits.argmax(dim=1).item()\n",
    "\n",
    "# Konversi kembali ke label asli\n",
    "label = le.inverse_transform([pred])[0]\n",
    "print(\"Predicted label:\", label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
